{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ur8VsJR3nRBG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GRAPH = {\n",
        "    0: [1, 3],\n",
        "    1: [0, 2, 4],\n",
        "    2: [1, 4],\n",
        "    3: [0, 4],\n",
        "    4: [1, 2, 3]\n",
        "}\n",
        "N_STATES = len(GRAPH)\n",
        "N_ACTIONS = N_STATES\n",
        "START_STATE = 0\n",
        "GOAL_STATE = 4\n",
        "POSITIVE_REWARD = 100\n",
        "NEGATIVE_REWARD = -1"
      ],
      "metadata": {
        "id": "ktdHoa5QnU_i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_reward(current_state, next_state):\n",
        "    if next_state == GOAL_STATE:\n",
        "        return POSITIVE_REWARD\n",
        "    if next_state not in GRAPH.get(current_state, []):\n",
        "        return NEGATIVE_REWARD * 10\n",
        "    return NEGATIVE_REWARD"
      ],
      "metadata": {
        "id": "8gv0uwEznYyY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Tabular Reinforcement Learning (Q-Learning) ---\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, states, actions, lr=0.1, gamma=0.9, epsilon=1.0, min_epsilon=0.01, decay_rate=0.9995):\n",
        "        self.states = states\n",
        "        self.actions = actions\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.decay_rate = decay_rate\n",
        "        self.q_table = np.zeros((states, actions))\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(GRAPH[state])\n",
        "        else:\n",
        "            q_values = self.q_table[state]\n",
        "            valid_actions = GRAPH[state]\n",
        "            best_action = valid_actions[np.argmax(q_values[valid_actions])]\n",
        "            return best_action\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        action_index = action\n",
        "        valid_next_actions = GRAPH.get(next_state, [])\n",
        "        if next_state == GOAL_STATE or not valid_next_actions:\n",
        "            max_next_q = 0\n",
        "        else:\n",
        "            max_next_q = np.max(self.q_table[next_state][valid_next_actions])\n",
        "\n",
        "        old_value = self.q_table[state, action_index]\n",
        "        new_value = (1 - self.lr) * old_value + self.lr * (reward + self.gamma * max_next_q)\n",
        "        self.q_table[state, action_index] = new_value\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon * self.decay_rate)\n"
      ],
      "metadata": {
        "id": "4GEufn6wnYwI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_q_learning(episodes=1000):\n",
        "    agent = QLearningAgent(N_STATES, N_ACTIONS)\n",
        "    total_steps = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "    for episode in range(episodes):\n",
        "        state = START_STATE\n",
        "        step_count = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state = action\n",
        "            reward = get_reward(state, next_state)\n",
        "\n",
        "            agent.update_q_table(state, action, reward, next_state)\n",
        "\n",
        "            state = next_state\n",
        "            total_steps += 1\n",
        "            step_count += 1\n",
        "\n",
        "            if state == GOAL_STATE or step_count > N_STATES * 5:\n",
        "                done = True\n",
        "\n",
        "        agent.decay_epsilon()\n",
        "\n",
        "    end_time = time.time()\n",
        "    return end_time - start_time, total_steps, agent.q_table"
      ],
      "metadata": {
        "id": "vMhAbAo4nYto"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "efuYEZAfnYfg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QTVFWwqUnYdJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook implements a basic Q-Learning agent to find the optimal path in a predefined graph. It covers the following key aspects:\n",
        "\n",
        "Environment Setup: Defines a graph (Markov Decision Process) with states and possible transitions, along with start and goal states and associated rewards.\n",
        "Reward Function: A get_reward function calculates rewards based on state transitions, including positive rewards for reaching the goal and negative rewards for invalid or non-goal moves.\n",
        "Q-Learning Agent: The QLearningAgent class implements the core Q-learning algorithm. It includes:\n",
        "Initialization: Sets up the Q-table, learning rate (lr), discount factor (gamma), and exploration-exploitation parameters (epsilon).\n",
        "Action Selection: Uses an epsilon-greedy policy to choose actions, balancing exploration (random actions) and exploitation (choosing actions with highest Q-value).\n",
        "Q-table Update: Implements the Q-learning update rule to iteratively improve action-value estimates.\n",
        "Epsilon Decay: Gradually reduces epsilon over time to shift from exploration to exploitation.\n",
        "Training Loop: The run_q_learning function orchestrates the training process for a specified number of episodes, simulating interactions with the environment, updating the Q-table, and tracking performance metrics like training time and total steps. It also handles episode termination conditions (reaching goal or max steps).\n",
        "The notebook provides a foundational example of reinforcement learning using Q-Learning for pathfinding in a simple discrete environment."
      ],
      "metadata": {
        "id": "Kvg8Jo_8c6QV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9qHmbtPdc62p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}